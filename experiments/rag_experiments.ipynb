{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "# **`Rag Experiments`**\n",
    "\n",
    "- In this notebook we will perform some experiments related to langchain and RAG.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders import (\n",
    "    WebBaseLoader,\n",
    "    PyPDFLoader,\n",
    "    Docx2txtLoader,\n",
    ")\n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = [\n",
    "    \"docs/DeepSeek_R1.pdf\",\n",
    "    \"docs/3_months_data_scientist_Roadmap.md\",\n",
    "    \"docs/Skin_Disease_Detection_Project_Description.docx\",\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for doc_file in doc_path:\n",
    "    file_path = Path(doc_file)\n",
    "\n",
    "    try:\n",
    "        \n",
    "        if doc_file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        \n",
    "        elif doc_file.endswith(\".docx\"):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "\n",
    "        elif doc_file.endswith(\".txt\") or doc_file.endswith(\".md\"):\n",
    "            loader = TextLoader(file_path)\n",
    "\n",
    "        else:\n",
    "            print(f\"\\nDocument Type {doc_file.type} not Supported.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError While Loading Document {doc_file.name}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load URL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://docs.streamlit.io/develop/quick-reference/release-notes\"\n",
    "\n",
    "try:\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError While Loading Document from {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release notes - Streamlit DocsDocumentationsearchSearchrocket_launchGet startedInstallationaddFundamentalsaddFirst stepsaddcodeDevelopConceptsaddAPI referenceaddTutorialsaddQuick referenceremoveCheat sheetRelease notesremove2025202420232022202120202019Pre-release featuresRoadmapopen_in_newweb_assetDeployConceptsaddStreamlit Community CloudaddSnowflakeOther platformsaddschoolKnowledge baseFAQInstalling dependenciesDeployment issuesHome/Develop/Quick \n"
     ]
    }
   ],
   "source": [
    "# load first 453 words of the document\n",
    "text = docs[0].page_content[:453]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split Documents into Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 5000,\n",
    "    chunk_overlap = 1000,\n",
    ")\n",
    "\n",
    "document_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenize and Load the Documents to the VectorStore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(\n",
    "    documents = document_chunks,\n",
    "    embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define Retriver To get the Content From VectorStore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_context_retriver_chain(vector_db,llm):\n",
    "    retriver = vector_db.as_retriever()\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name = \"messages\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation, generate a search query to look up in order to get inforamtion relevant to the conversation, focusing on the most recent messages.\"),\n",
    "    ])\n",
    "    retriver_chain = create_history_aware_retriever(llm, retriver, prompt)\n",
    "    return retriver_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define a Conversational RAG Chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversational_rag_chain(llm):\n",
    "    retriver_chain = _get_context_retriver_chain(vector_db, llm)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "         \"\"\"You are a Helpfull Assistant. You will have to answer the user's Queries.\n",
    "         You will have Some Context to Help With Your Aanswers, but now always would be compleatly related or Helpfull.\n",
    "         You can also use your Knowledge to assist answering the User's Qqueries.\\n\n",
    "         {context}\"\"\"),\n",
    "        MessagesPlaceholder(variable_name = \"messages\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ])\n",
    "    stuff_documents_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    return create_retrieval_chain(retriver_chain, stuff_documents_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Augmented Response Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about the latest version of Streamlit. I remember seeing the release notes earlier. Let me check the context provided. \n",
      "\n",
      "In the context, under the release notes, it mentions version 1.42.0 with a release date of February 4, 2025. That's the latest one. \n",
      "\n",
      "I should confirm that this is indeed the most recent version. Also, the user might be looking to upgrade, so including the upgrade command would be helpful. \n",
      "\n",
      "I should present the version clearly and maybe add a tip on how to upgrade using pip. That way, the user gets both the information and the next steps if they need to update.\n",
      "</think>\n",
      "\n",
      "The latest version of Streamlit is **1.42.0**, released on **February 4, 2025**. You can upgrade to this version using pip:\n",
      "\n",
      "```bash\n",
      "pip install --upgrade streamlit```"
     ]
    }
   ],
   "source": [
    "llm_stream_groq = ChatGroq(\n",
    "    model = \"deepseek-r1-distill-llama-70b\",\n",
    "    temperature = 0.3,\n",
    "    streaming = True,\n",
    ")\n",
    "\n",
    "llm_stream_gemini = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-1.5-flash\",\n",
    "    temperature = 0.3,\n",
    ")\n",
    "\n",
    "llm_stream = llm_stream_groq # select between groq and gemini for response\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the latest version of Streamlit?\"},\n",
    "]\n",
    "\n",
    "messages = [HumanMessage(content=m[\"content\"]) if m[\"role\"] == \"user\" else AIMessage(content=m[\"content\"]) for m in messages]\n",
    "\n",
    "conversation_rag_chain = get_conversational_rag_chain(llm_stream)\n",
    "\n",
    "response_message = \"**RAG Response**\\n\"\n",
    "\n",
    "for chunk in conversation_rag_chain.pick(\"answer\").stream({\"messages\" : messages[:-1], \"input\" : messages[-1].content}):\n",
    "    response_message += chunk\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "messages.append({\"role\" : \"assistant\", \"content\" : response_message})  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
